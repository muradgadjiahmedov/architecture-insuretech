# InsureTech — Задание 3. Переход на Event‑Driven архитектуру
**Контекст:** сейчас `core-app` и `ins-comp-settlement` тянут продукты/тарифы синхронно из `ins-product-aggregator` (REST). `core-app` — каждые 15 минут, `ins-comp-settlement` — раз в сутки. В ближайшее время добавляются ещё 5 страховых компаний (в сумме 10).

---

## 1) Проблемы текущей архитектуры (as‑is)
1. **Сильная синхронная связность и фан‑аут:** один REST‑запрос в `ins-product-aggregator` приводит к N (сейчас 5, скоро 10) внешним запросам к страховщикам → рост латентности, спайки нагрузки, «каскадные таймауты».  
2. **Периодический polling → несвежие данные:** локальные реплики в `core-app` и `ins-comp-settlement` устаревают до следующего окна опроса (15 минут / 24 часа).  
3. **Задержки/ошибки у страховщиков «проливаются» вверх:** SLA и UX деградируют из‑за медленных/падающих внешних API.  
4. **Повторяющийся трафик и дублирование логики агрегации:** оба сервиса сами решают, когда тянуть данные. Это размножает расписания, ретраи, кэш‑стратегии и точки отказа.  
5. **Непредсказуемая нагрузка на агрегатор:** одновременно может прийти много запросов (веб + партнёры + ночные джобы), что приводит к **контеншену ресурсов** и нестабильности.  
6. **Днём «короткие пики», ночью «длинные пики»:** ночной регламент `ins-comp-settlement` бьёт в «узкое горлышко» сразу после полуночи → шипы RPS/RT/CPU/IO.  
7. **REST‑вытягивание оформленных полисов `ins-comp-settlement -> core-app` раз в сутки:** крупные объёмы + возможные таймауты/повторы → риск пропусков/дубликатов.  
8. **Сложность масштабирования под рост поставщиков:** добавление ещё 5 интеграций пропорционально увеличит длительность и нестабильность синхронных запросов.  
9. **Сложность тестирования и воспроизводимости инцидентов:** без чётких событий и журналов «какой апдейт пришёл когда» трудно отлаживать.  
10. **Ограниченная наблюдаемость и причинно‑следственные связи:** нет сквозной трассировки событий «обновился тариф → появился в витрине → оформлен полис → ушёл в реестр».

---

## 2) Риски при росте (10 страховщиков, рекламная кампания)
- **Каскадные отказы и «шторм» ретраев.**  
- **Скачок TTFB и ошибок 5xx** на страницах каталога/оформления.  
- **Срыв SLA B2B** при всплесках от партнёров.  
- **Несогласованность данных** между локальными репликами (разные «срезы времени»).  
- **Удорожание ресурсов** (противоестественное вертикальное масштабирование без эффекта).  
- **Сложность онбординга новых интеграций** (рост времени вывода фич).

---

## 3) Целевая архитектура (to‑be): Event‑Driven + Streaming
**Идея:** перевести изменения продуктов/тарифов и «события бизнес‑жизни» в доменные события, публикуемые в шину (Kafka/Redpanda). Сервисы **подписываются** и **реагируют** асинхронно, локальные реплики поддерживаются **push‑моделью**.

### 3.1. Основные темы (topics) и события
- `products.catalog.updates` — изменения справочника продуктов и атрибутов (ключ: `product_id` / `insurer_id`).  
- `products.tariffs.updates` — изменения тарифов/цен/условий (ключ: `tariff_id` / `insurer_id`).  
- `policy.issued` — оформлен полис (ключ: `policy_id`, payload включает id клиента и продукта).  
- `policy.cancelled` — аннулирован полис.  
- `settlement.reports.requested` — запрос на формирование реестра (инициирует пакетную обработку).  
- `settlement.reports.ready` — реестр готов (ссылка на объект в хранилище, версия).  
- `aggregator.fetch.failed` / `aggregator.fetch.slow` — тех. события качества интеграций (для SRE/оповещений).  
- `dlq.*` — очереди мёртвых сообщений для «ядовитых» событий.

### 3.2. Роли сервисов
- **ins-product-aggregator**  
  - Периодически/по вебхукам **вытягивает изменения у страховщиков**, нормализует и **публикует** события в `products.*.updates`.  
  - Не держит длинных синхронных объединений под фронтовые запросы.  
  - Поддерживает **идемпотентность** (natural keys + версионирование) и **outbox** (см. ниже).
- **core-app**  
  - **Подписывается** на `products.*.updates`, **держит локальные проекции** (read‑модели) в своей БД/кэше для быстрой витрины и расчёта.  
  - На оформление полиса **порождает** `policy.issued`.  
- **ins-comp-settlement**  
  - **Подписывается** на `policy.issued` (получает транзакции в режиме near‑real‑time) и на `products.*.updates` (для актуальных справочников).  
  - По расписанию/запросу публикует `settlement.reports.requested`, асинхронно формирует отчёт и публикует `settlement.reports.ready`.
- **Schema Registry + DLQ + Observability** для эволюции контрактов и устойчивости пайплайна.

### 3.3. Гарантии доставки
- Kafka с **acks=all**, **min.insync.replicas>=2**, ретраи продюсера; потребители с **идемпотентной обработкой** (проверка версии/offset, upsert).  
- Для критичных потоков возможен **Exactly‑Once Semantics** (EOS) в пределах Kafka + idempotent producer + транзакционные потребители.  
- **DLQ** и **replay** по ключу/интервалу (retention ≥ 7–30 дней).

### 3.4. Кэш и проекции
- Локальные проекции в `core-app` и `ins-comp-settlement` обновляются «событиями», а не polling‑ом.  
- Холодный старт — **bootstrap**: initial загрузка снапшота из `ins-product-aggregator` (bulk) + догон по хвосту топика по offset/времени.

---

## 4) Transactional Outbox — применять? → **ДА**
**Где:** минимум в `core-app` (на `policy.issued`) и `ins-product-aggregator` (на `products.*.updates`).  
**Зачем:** чтобы **атомарно** фиксировать локальную запись (в БД) и публикацию события.  
**Как:** таблица `outbox_events` в той же БД, запись события в одной транзакции с бизнес‑данными → асинхронный **Outbox‑релизер** публикует в Kafka.  
**Альтернатива/расширение:** CDC (Debezium) — можно подключить позже для масштабных потоков.

**Схемы outbox:**  
- `event_id (uuid)`, `aggregate_type`, `aggregate_id`, `type`, `payload (jsonb)`, `version`, `created_at`, `status`, `retry_count`.  
- Публикация с ключом `aggregate_id`, семантика UPSERT на стороне консюмера.

---

## 5) Идемпотентность и порядок
- Ключи по доменной идентичности (`policy_id`, `product_id`, `tariff_id`).  
- Версионирование (`version`, `seq`, `updated_at`) и **last‑write‑wins** или оптимистичные локации.  
- «Безопасные» ретраи: повторная обработка события не меняет итог (UPSERT).

---

## 6) Обновлённый SLA/SLO и SRE‑практики
- **SLO каталога:** P95 TTFB витрины ≤ 300‑500 мс за счёт локальных проекций.  
- **SLO публикации:** `products.*.updates` до проставления в read‑модель ≤ 1–2 минуты при норме.  
- **Алерты:** лаг консюмера > 5 минут; рост DLQ; ретраи продюсера; деградация внешних интеграций `aggregator.fetch.*`.

---

## 7) План миграции по шагам (safe rollout)
1. Добавить Kafka/Redpanda, Schema Registry, DLQ, Observability.  
2. В `ins-product-aggregator` реализовать outbox и публикацию `products.*.updates` (параллельно оставить REST).  
3. В `core-app` и `ins-comp-settlement` — консюмеры + read‑модели; запустить bootstrap снапшота.  
4. Перевести `ins-comp-settlement` с REST‑pull `core-app` на `policy.issued` (event‑push).  
5. Постепенно уменьшать частоту polling и/или отключить.  
6. Для отчётов внедрить `settlement.reports.*` как асинхронный процесс.  
7. Decommission устаревших cron‑ов, лимитировать синхронные REST путём circuit‑breaker/rate‑limit.  

---

## 8) Выгоды целевого решения
- **Снижение латентности витрины** (данные рядом, без «фан‑аута» на критическом пути).  
- **Устойчивость к деградации страховщиков** (асинхронное сглаживание).  
- **Масштабируемость по числу интеграций** (просто больше событий, а не больше скачков TTFB).  
- **Надёжная доставка и воспроизводимость** (лог событий, реплей, DLQ).  
- **Прозрачность потоков** (сквозная трассировка «событие → проекция → отчёт»).
